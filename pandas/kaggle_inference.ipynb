{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENT_SAMPLING = 0.1\n",
    "BASELINE = False\n",
    "MODEL_LOAD_DIR = './models'\n",
    "BATCH_SIZE = 15\n",
    "PATCH_SIZE = 128\n",
    "SCALE_FACTOR = 4\n",
    "IMAGE_SIZE = 512*SCALE_FACTOR\n",
    "WARMUP_EPOCHS = 2\n",
    "PATCH_BATCHES = math.ceil(1/PERCENT_SAMPLING)\n",
    "INNER_ITERATION = PATCH_BATCHES\n",
    "ACCELARATOR = \n",
    "LATENT_DIMENSION = 256\n",
    "NUM_CLASSES = 6\n",
    "SEED = 42\n",
    "STRIDE = PATCH_SIZE\n",
    "NUM_PATCHES = ((IMAGE_SIZE-PATCH_SIZE)//STRIDE) + 1\n",
    "NUM_WORKERS = 4\n",
    "TRAIN_ROOT_DIR = f'..\\\\data\\\\pandas_dataset\\\\training_images_{IMAGE_SIZE}'\n",
    "VAL_ROOT_DIR = TRAIN_ROOT_DIR\n",
    "TRAIN_CSV_PATH = f'..\\\\data\\\\pandas_dataset\\\\train_kfold.csv'\n",
    "MEAN = [0.9770, 0.9550, 0.9667]\n",
    "STD = [0.0783, 0.1387, 0.1006]\n",
    "MODEL_SAVE_DIR = f\"./\"\n",
    "DECAY_FACTOR = 1\n",
    "VALIDATION_EVERY = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_resolution_index(io_object,dimension):\n",
    "    best_index = len(io_object) - 1\n",
    "    for i in reversed(range(len(io_object))):\n",
    "        shape = io_object[i].shape\n",
    "        if dimension > min(shape[0],shape[1]):\n",
    "            break\n",
    "        best_index = i\n",
    "    return best_index\n",
    "\n",
    "\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self,df,root_dir,transforms=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,index):\n",
    "        image_id = self.df.iloc[index].image_id\n",
    "        path = glob(self.root_dir+f'\\\\{image_id}.tiff')\n",
    "        biopsy = skimage.io.MultiImage(path)\n",
    "        best_index = get_best_resolution_index(biopsy,IMAGE_SIZE)\n",
    "        label = self.df.iloc[index].isup_grade\n",
    "        image = cv2.imread(f\"{self.root_dir}/{image_id}.png\")\n",
    "        im = biopsy[best_index]\n",
    "        old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "        ratio = float(IMAGE_SIZE)/max(old_size)\n",
    "        new_size = tuple([int(x*ratio) for x in old_size])\n",
    "        # new_size should be in (width, height) format\n",
    "        im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "        delta_w = IMAGE_SIZE - new_size[1]\n",
    "        delta_h = IMAGE_SIZE - new_size[0]\n",
    "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "        color = [255, 255, 255]\n",
    "        new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self,images,num_patches,stride,patch_size):\n",
    "        self.images = images\n",
    "        self.num_patches = num_patches\n",
    "        self.stride = stride\n",
    "        self.patch_size = patch_size\n",
    "    def __len__(self):\n",
    "        return self.num_patches ** 2\n",
    "    def __getitem__(self,choice):\n",
    "        i = choice%self.num_patches\n",
    "        j = choice//self.num_patches\n",
    "        return self.images[:,:,self.stride*i:self.stride*i+self.patch_size,self.stride*j:self.stride*j+self.patch_size], choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self,baseline,latent_dim):\n",
    "        super(Backbone,self).__init__()\n",
    "        # self.encoder = timm.create_model('resnet10t',pretrained=True)\n",
    "        self.encoder = resnet50(pretrained=True)\n",
    "        if baseline:\n",
    "            self.encoder.fc = nn.Linear(2048,NUM_CLASSES)\n",
    "        else:\n",
    "            self.encoder.fc = nn.Linear(2048,latent_dim)\n",
    "    def forward(self,x):\n",
    "        return self.encoder(x)\n",
    " \n",
    "class CNN_Block(nn.Module):\n",
    "    def __init__(self,latent_dim,num_classes,num_patches):\n",
    "        super(CNN_Block,self).__init__()\n",
    "        self.expected_dim = (2,latent_dim,num_patches,num_patches)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim,latent_dim,3,1,1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim,latent_dim,3,2,1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim,latent_dim,3,2,1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim,latent_dim,3,2,1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        flatten_dim = self.get_final_out_dimension(self.expected_dim)\n",
    "        self.linear = nn.Linear(flatten_dim,num_classes)\n",
    "\n",
    "    def get_output_shape(self, model, image_dim):\n",
    "        return model(torch.rand(*(image_dim))).data.shape\n",
    "\n",
    "    def get_final_out_dimension(self,shape):\n",
    "        s = shape\n",
    "        s = self.get_output_shape(self.layer1,s)\n",
    "        s = self.get_output_shape(self.layer2,s)\n",
    "        s = self.get_output_shape(self.layer3,s)\n",
    "        s = self.get_output_shape(self.layer4,s)\n",
    "        return np.prod(list(s[1:]))\n",
    "\n",
    "    def forward(self,x,print_shape=False):\n",
    "        x = self.layer1(x)\n",
    "        if print_shape:\n",
    "            print(x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        if print_shape:\n",
    "            print(x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        if print_shape:\n",
    "            print(x.size())\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer4(x)\n",
    "        if print_shape:\n",
    "            print(x.size())\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x = self.linear(x)\n",
    "        if print_shape:\n",
    "            print(x.size())\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def make_baseline_inference():\n",
    "    torch.cuda.empty_cache()\n",
    "    model = Backbone(True,0)\n",
    "    model.eval()\n",
    "    model.to(ACCELARATOR)\n",
    "    timing = []\n",
    "\n",
    "    inputs = torch.rand(batch_size,3,image_size,image_size)\n",
    "    inputs = inputs.to(ACCELARATOR)\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(WARMUP):\n",
    "        model(inputs)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    for _ in range(NUM_ITERATIONS):\n",
    "        start = time.time()\n",
    "        model(inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        timing.append(time.time() - start)\n",
    "\n",
    "    timing = torch.as_tensor(timing, dtype=torch.float32)\n",
    "    return batch_size / timing.mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5b343e82b267508e42772a71539969b6186dad596e9eab09063586cda2d77b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
